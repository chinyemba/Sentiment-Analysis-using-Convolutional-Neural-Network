{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwB_Jq-5PjnU",
        "colab_type": "code",
        "outputId": "c9bd2a02-13f9-4260-b0e4-e79b1643f8ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fsu02OfM1g4L",
        "colab_type": "code",
        "outputId": "4b7952eb-ffdd-4a7a-ae81-1710238b3dc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        }
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "!pip install tensorflow-hub\n",
        "!pip install tfds-nightly\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "from google.colab import files\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow import keras\n",
        "import os\n",
        "import io\n",
        "import re\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "print(\"Version: \", tf.__version__)\n",
        "print(\"Eager mode: \", tf.executing_eagerly())\n",
        "print(\"Hub version: \", hub.__version__)\n",
        "print(\"GPU is\", \"available\" if tf.config.experimental.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.6/dist-packages (0.7.0)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (1.18.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (3.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.4.0->tensorflow-hub) (46.0.0)\n",
            "Collecting tfds-nightly\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/09/3be889b6ef8424273d10a03b206d933290e9148d82c9cf97ed1cee7dbdcd/tfds_nightly-2.1.0.dev202003300105-py3-none-any.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 16.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (0.16.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (0.9.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (4.38.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (0.3.1.1)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (2.3)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (3.10.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (0.21.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (1.12.0)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (19.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (1.18.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (2.21.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (1.12.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tfds-nightly) (46.0.0)\n",
            "Requirement already satisfied: googleapis-common-protos in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata->tfds-nightly) (1.51.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tfds-nightly) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tfds-nightly) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tfds-nightly) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tfds-nightly) (2019.11.28)\n",
            "Installing collected packages: tfds-nightly\n",
            "Successfully installed tfds-nightly-2.1.0.dev202003300105\n",
            "Version:  2.2.0-rc1\n",
            "Eager mode:  True\n",
            "Hub version:  0.7.0\n",
            "GPU is available\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9hHHJVo6xMM",
        "colab_type": "code",
        "outputId": "e653a1e3-867c-4413-9e75-06d1969c3a5c",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7b615d9c-c4bd-4b05-b068-9df44e76efe9\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-7b615d9c-c4bd-4b05-b068-9df44e76efe9\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving uglywords.csv to uglywords.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dz8F5FQlE1U8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbrWvvCwFLF1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv(io.BytesIO(uploaded['uglywords.csv']), encoding = 'latin1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcHqVBV4FYxF",
        "colab_type": "code",
        "outputId": "55ebbb97-355e-4b9b-a565-5dec629da4b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "source": [
        "data.head(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label</th>\n",
              "      <th>Tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>*screams in 25 different languages*</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>Families to sue over Legionnaires: More than 4...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>Pandemonium In Aba As Woman Delivers Baby With...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>My emotions are a train wreck. My body is a tr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>Alton brown just did a livestream and he burne...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1.0</td>\n",
              "      <td>@TinyJecht Are you another Stand-user? If you ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1.0</td>\n",
              "      <td>brooke just face timed me at the concert and j...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.0</td>\n",
              "      <td>A group of Florida Forest Service firefighters...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.0</td>\n",
              "      <td>70 Years After Atomic Bombs Japan Still Strugg...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.0</td>\n",
              "      <td>The majority of those killed were civilians on...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Label                                              Tweet\n",
              "0    1.0                *screams in 25 different languages*\n",
              "1    0.0  Families to sue over Legionnaires: More than 4...\n",
              "2    0.0  Pandemonium In Aba As Woman Delivers Baby With...\n",
              "3    0.0  My emotions are a train wreck. My body is a tr...\n",
              "4    1.0  Alton brown just did a livestream and he burne...\n",
              "5    1.0  @TinyJecht Are you another Stand-user? If you ...\n",
              "6    1.0  brooke just face timed me at the concert and j...\n",
              "7    0.0  A group of Florida Forest Service firefighters...\n",
              "8    0.0  70 Years After Atomic Bombs Japan Still Strugg...\n",
              "9    0.0  The majority of those killed were civilians on..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xX6qAVUrFwQF",
        "colab_type": "code",
        "outputId": "cda9efbe-cf98-408a-9436-7d547c101c2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(739, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vN1n9QXEYqJv",
        "colab_type": "code",
        "outputId": "d7133ca2-1001-4a3e-8d24-61b462b69217",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "list(data.columns.values)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Label', 'Tweet']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p--3IP-vYylt",
        "colab_type": "code",
        "outputId": "b53546e9-6e82-4c85-e096-9f7a0fc99f41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "#lets see the count by type of sentiments\n",
        "\n",
        "print((data.Label.value_counts()/sum(data.Label.value_counts()))*100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0    63.821138\n",
            "1.0    36.178862\n",
            "Name: Label, dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yKuMZF_Y--d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Tokenize the text\n",
        "import re\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "wordnet_lemmatizer = WordNetLemmatizer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0YZoxIFZJ2c",
        "colab_type": "code",
        "outputId": "2b5f8c9b-c8cb-439b-d056-980e35f5a973",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "import nltk\n",
        "nltk.download('punkt')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZfsbbeDZTgc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define a function to use to clean the data\n",
        "def normalizer(tweet):\n",
        "    no_urls = re.sub(r\"http\\S+\", \" \" ,tweet)\n",
        "    only_letters = re.sub(\"[^a-zA-Z]\", \" \",no_urls)\n",
        "    tokens = nltk.word_tokenize(only_letters)[2 :]\n",
        "    lower_case = [l.lower() for l in tokens]\n",
        "    filtered_result = list(filter(lambda l: l not in stop_words, lower_case))\n",
        "    lemmas = [wordnet_lemmatizer.lemmatize(t) for t in filtered_result]\n",
        "    return lemmas"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kR86nKrxZcDc",
        "colab_type": "code",
        "outputId": "779091d2-5544-4539-f6d4-c7e8988c92ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "normalizer('Shamiso is so dangerous because you never know what her emotions mean')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['dangerous', 'never', 'know', 'emotion', 'mean']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_k8fQt0Zk_r",
        "colab_type": "code",
        "outputId": "6868e6fc-50ab-443b-98dd-c9c3a5d248cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        }
      },
      "source": [
        "#apply the function to clean the data:\n",
        "pd.set_option('display.max_colwidth', -1)\n",
        "data['normalized_tweet'] = data.Tweet.apply(normalizer)\n",
        "data[['Tweet', 'normalized_tweet']].head(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tweet</th>\n",
              "      <th>normalized_tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>*screams in 25 different languages*</td>\n",
              "      <td>[different, language]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Families to sue over Legionnaires: More than 40 families affected by the fatal outbreak of Legionnaires' disea... http://t.co/ZA4AXFJSVB</td>\n",
              "      <td>[sue, legionnaire, family, affected, fatal, outbreak, legionnaire, disea]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Pandemonium In Aba As Woman Delivers Baby Without Face (Photos) - http://t.co/c5u9qshhnb</td>\n",
              "      <td>[aba, woman, delivers, baby, without, face, photo]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>My emotions are a train wreck. My body is a train wreck. I'm a wreck</td>\n",
              "      <td>[train, wreck, body, train, wreck, wreck]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Alton brown just did a livestream and he burned the butter and touched the hot plate too soon and made a nut joke http://t.co/gvd7fcx8iZ</td>\n",
              "      <td>[livestream, burned, butter, touched, hot, plate, soon, made, nut, joke]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>@TinyJecht Are you another Stand-user? If you are I will have to detonate you with my Killer Queen.</td>\n",
              "      <td>[another, stand, user, detonate, killer, queen]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>brooke just face timed me at the concert and just screamed for 2 minutes straight</td>\n",
              "      <td>[face, timed, concert, screamed, minute, straight]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>A group of Florida Forest Service firefighters could be deployed to California to help contain fires. Details at 10! http://t.co/fwuP9YURzY</td>\n",
              "      <td>[florida, forest, service, firefighter, could, deployed, california, help, contain, fire, detail]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>70 Years After Atomic Bombs Japan Still Struggles With War Past: The anniversary of the devastation wrought b... http://t.co/iTBJ6DKRZI</td>\n",
              "      <td>[atomic, bomb, japan, still, struggle, war, past, anniversary, devastation, wrought, b]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>The majority of those killed were civilians on the ground after the jet first bombed the city's main street then dramatically plummeted</td>\n",
              "      <td>[killed, civilian, ground, jet, first, bombed, city, main, street, dramatically, plummeted]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                         Tweet                                                                                   normalized_tweet\n",
              "0  *screams in 25 different languages*                                                                                                          [different, language]                                                                            \n",
              "1  Families to sue over Legionnaires: More than 40 families affected by the fatal outbreak of Legionnaires' disea... http://t.co/ZA4AXFJSVB     [sue, legionnaire, family, affected, fatal, outbreak, legionnaire, disea]                        \n",
              "2  Pandemonium In Aba As Woman Delivers Baby Without Face (Photos) - http://t.co/c5u9qshhnb                                                     [aba, woman, delivers, baby, without, face, photo]                                               \n",
              "3  My emotions are a train wreck. My body is a train wreck. I'm a wreck                                                                         [train, wreck, body, train, wreck, wreck]                                                        \n",
              "4  Alton brown just did a livestream and he burned the butter and touched the hot plate too soon and made a nut joke http://t.co/gvd7fcx8iZ     [livestream, burned, butter, touched, hot, plate, soon, made, nut, joke]                         \n",
              "5  @TinyJecht Are you another Stand-user? If you are I will have to detonate you with my Killer Queen.                                          [another, stand, user, detonate, killer, queen]                                                  \n",
              "6  brooke just face timed me at the concert and just screamed for 2 minutes straight                                                            [face, timed, concert, screamed, minute, straight]                                               \n",
              "7  A group of Florida Forest Service firefighters could be deployed to California to help contain fires. Details at 10! http://t.co/fwuP9YURzY  [florida, forest, service, firefighter, could, deployed, california, help, contain, fire, detail]\n",
              "8  70 Years After Atomic Bombs Japan Still Struggles With War Past: The anniversary of the devastation wrought b... http://t.co/iTBJ6DKRZI      [atomic, bomb, japan, still, struggle, war, past, anniversary, devastation, wrought, b]          \n",
              "9  The majority of those killed were civilians on the ground after the jet first bombed the city's main street then dramatically plummeted      [killed, civilian, ground, jet, first, bombed, city, main, street, dramatically, plummeted]      "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zo_PZ4kgaR8Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test = 'https://www.shatorangola.com is the best company where you can buy all your PPE and get help with visual inspection whenever you need it%?//267'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAWmv-UHbkxl",
        "colab_type": "code",
        "outputId": "828638ec-0d38-4d10-fdc6-2531c975975b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(normalizer(test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['best', 'company', 'buy', 'ppe', 'get', 'help', 'visual', 'inspection', 'whenever', 'need']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFtXRvj4G5JX",
        "colab_type": "code",
        "outputId": "1fe9b968-8448-4457-c707-3865c894c5b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label</th>\n",
              "      <th>Tweet</th>\n",
              "      <th>normalized_tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>*screams in 25 different languages*</td>\n",
              "      <td>[different, language]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>Families to sue over Legionnaires: More than 40 families affected by the fatal outbreak of Legionnaires' disea... http://t.co/ZA4AXFJSVB</td>\n",
              "      <td>[sue, legionnaire, family, affected, fatal, outbreak, legionnaire, disea]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>Pandemonium In Aba As Woman Delivers Baby Without Face (Photos) - http://t.co/c5u9qshhnb</td>\n",
              "      <td>[aba, woman, delivers, baby, without, face, photo]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>My emotions are a train wreck. My body is a train wreck. I'm a wreck</td>\n",
              "      <td>[train, wreck, body, train, wreck, wreck]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>Alton brown just did a livestream and he burned the butter and touched the hot plate too soon and made a nut joke http://t.co/gvd7fcx8iZ</td>\n",
              "      <td>[livestream, burned, butter, touched, hot, plate, soon, made, nut, joke]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Label  ...                                                           normalized_tweet\n",
              "0  1.0    ...  [different, language]                                                    \n",
              "1  0.0    ...  [sue, legionnaire, family, affected, fatal, outbreak, legionnaire, disea]\n",
              "2  0.0    ...  [aba, woman, delivers, baby, without, face, photo]                       \n",
              "3  0.0    ...  [train, wreck, body, train, wreck, wreck]                                \n",
              "4  1.0    ...  [livestream, burned, butter, touched, hot, plate, soon, made, nut, joke] \n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiGUmYTrKLMm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Now we shall look at the words that contributed to each type of sentiment more than the other"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajTq8430MoZf",
        "colab_type": "code",
        "outputId": "2c349223-3a0e-4cb6-db86-24170e87e471",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "from nltk import ngrams\n",
        "def ngrams(input_list):\n",
        "    bigrams = [' '.join(t) for t in list(zip(input_list, input_list[1:]))]\n",
        "    trigrams = [' '.join(t) for t in list(zip(input_list, input_list[1:], input_list[2:]))]\n",
        "    return bigrams + trigrams\n",
        "data['grams'] = data.normalized_tweet.apply(ngrams)\n",
        "data[['grams']].head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>grams</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[different language]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[sue legionnaire, legionnaire family, family affected, affected fatal, fatal outbreak, outbreak legionnaire, legionnaire disea, sue legionnaire family, legionnaire family affected, family affected fatal, affected fatal outbreak, fatal outbreak legionnaire, outbreak legionnaire disea]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[aba woman, woman delivers, delivers baby, baby without, without face, face photo, aba woman delivers, woman delivers baby, delivers baby without, baby without face, without face photo]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[train wreck, wreck body, body train, train wreck, wreck wreck, train wreck body, wreck body train, body train wreck, train wreck wreck]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[livestream burned, burned butter, butter touched, touched hot, hot plate, plate soon, soon made, made nut, nut joke, livestream burned butter, burned butter touched, butter touched hot, touched hot plate, hot plate soon, plate soon made, soon made nut, made nut joke]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                                          grams\n",
              "0  [different language]                                                                                                                                                                                                                                                                        \n",
              "1  [sue legionnaire, legionnaire family, family affected, affected fatal, fatal outbreak, outbreak legionnaire, legionnaire disea, sue legionnaire family, legionnaire family affected, family affected fatal, affected fatal outbreak, fatal outbreak legionnaire, outbreak legionnaire disea]\n",
              "2  [aba woman, woman delivers, delivers baby, baby without, without face, face photo, aba woman delivers, woman delivers baby, delivers baby without, baby without face, without face photo]                                                                                                   \n",
              "3  [train wreck, wreck body, body train, train wreck, wreck wreck, train wreck body, wreck body train, body train wreck, train wreck wreck]                                                                                                                                                    \n",
              "4  [livestream burned, burned butter, butter touched, touched hot, hot plate, plate soon, soon made, made nut, nut joke, livestream burned butter, burned butter touched, butter touched hot, touched hot plate, hot plate soon, plate soon made, soon made nut, made nut joke]                "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCgyYnuwMsJ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import collections\n",
        "def count_words(input):\n",
        "    cnt = collections.Counter()\n",
        "    for row in input:\n",
        "        for word in row:\n",
        "            cnt[word] += 1\n",
        "    return cnt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvZkE03_NaPp",
        "colab_type": "code",
        "outputId": "a7eb2a40-76a7-4515-d521-5dc7903c8e81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "#word counts per label in from high to low\n",
        "data[(data.Label == 1)][['grams']].apply(count_words)['grams'].most_common(15)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('gon na', 4),\n",
              " ('forest fire', 3),\n",
              " ('full read', 3),\n",
              " ('body bag', 2),\n",
              " ('year old', 2),\n",
              " ('anybody else', 2),\n",
              " ('read ebay', 2),\n",
              " ('full read ebay', 2),\n",
              " ('issue issue', 2),\n",
              " ('syrian refugee', 2),\n",
              " ('quarantine offensive', 2),\n",
              " ('offensive content', 2),\n",
              " ('quarantine offensive content', 2),\n",
              " ('high school', 2),\n",
              " ('meat loving', 2)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_QAg6OGNdAv",
        "colab_type": "code",
        "outputId": "adff7463-8800-451d-84a3-feb00be1ca6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "#word counts per label in from high to low\n",
        "data[(data.Label == 0)][['grams']].apply(count_words)['grams'].most_common(15)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('airport get', 5),\n",
              " ('get swallowed', 5),\n",
              " ('swallowed sandstorm', 5),\n",
              " ('sandstorm minute', 5),\n",
              " ('airport get swallowed', 5),\n",
              " ('get swallowed sandstorm', 5),\n",
              " ('swallowed sandstorm minute', 5),\n",
              " ('oil spill', 5),\n",
              " ('mass murder', 4),\n",
              " ('northern california', 4),\n",
              " ('disaster typhoon', 4),\n",
              " ('typhoon devastated', 4),\n",
              " ('devastated saipan', 4),\n",
              " ('disaster typhoon devastated', 4),\n",
              " ('typhoon devastated saipan', 4)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMyxuJS3NnqY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#data = data.drop(['grams'], axis=1)\n",
        "data = data.drop(['Tweet'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHW_bEEQN0JI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.to_csv('clean_text.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfJmFVRsN8xv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#split dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtV2YCV8JOpi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trained_dataset_url = \"/content/clean_text.csv\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4L13HkQmJOvu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train = pd.read_csv(trained_dataset_url)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSjPyfr9P9Mh",
        "colab_type": "code",
        "outputId": "32866c79-6575-4ce2-a495-12645d76fed0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "df_train.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Label</th>\n",
              "      <th>normalized_tweet</th>\n",
              "      <th>grams</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>['different', 'language']</td>\n",
              "      <td>['different language']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>['sue', 'legionnaire', 'family', 'affected', 'fatal', 'outbreak', 'legionnaire', 'disea']</td>\n",
              "      <td>['sue legionnaire', 'legionnaire family', 'family affected', 'affected fatal', 'fatal outbreak', 'outbreak legionnaire', 'legionnaire disea', 'sue legionnaire family', 'legionnaire family affected', 'family affected fatal', 'affected fatal outbreak', 'fatal outbreak legionnaire', 'outbreak legionnaire disea']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>['aba', 'woman', 'delivers', 'baby', 'without', 'face', 'photo']</td>\n",
              "      <td>['aba woman', 'woman delivers', 'delivers baby', 'baby without', 'without face', 'face photo', 'aba woman delivers', 'woman delivers baby', 'delivers baby without', 'baby without face', 'without face photo']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>['train', 'wreck', 'body', 'train', 'wreck', 'wreck']</td>\n",
              "      <td>['train wreck', 'wreck body', 'body train', 'train wreck', 'wreck wreck', 'train wreck body', 'wreck body train', 'body train wreck', 'train wreck wreck']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>1.0</td>\n",
              "      <td>['livestream', 'burned', 'butter', 'touched', 'hot', 'plate', 'soon', 'made', 'nut', 'joke']</td>\n",
              "      <td>['livestream burned', 'burned butter', 'butter touched', 'touched hot', 'hot plate', 'plate soon', 'soon made', 'made nut', 'nut joke', 'livestream burned butter', 'burned butter touched', 'butter touched hot', 'touched hot plate', 'hot plate soon', 'plate soon made', 'soon made nut', 'made nut joke']</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ...                                                                                                                                                                                                                                                                                                                   grams\n",
              "0  0           ...  ['different language']                                                                                                                                                                                                                                                                                                \n",
              "1  1           ...  ['sue legionnaire', 'legionnaire family', 'family affected', 'affected fatal', 'fatal outbreak', 'outbreak legionnaire', 'legionnaire disea', 'sue legionnaire family', 'legionnaire family affected', 'family affected fatal', 'affected fatal outbreak', 'fatal outbreak legionnaire', 'outbreak legionnaire disea']\n",
              "2  2           ...  ['aba woman', 'woman delivers', 'delivers baby', 'baby without', 'without face', 'face photo', 'aba woman delivers', 'woman delivers baby', 'delivers baby without', 'baby without face', 'without face photo']                                                                                                       \n",
              "3  3           ...  ['train wreck', 'wreck body', 'body train', 'train wreck', 'wreck wreck', 'train wreck body', 'wreck body train', 'body train wreck', 'train wreck wreck']                                                                                                                                                            \n",
              "4  4           ...  ['livestream burned', 'burned butter', 'butter touched', 'touched hot', 'hot plate', 'plate soon', 'soon made', 'made nut', 'nut joke', 'livestream burned butter', 'burned butter touched', 'butter touched hot', 'touched hot plate', 'hot plate soon', 'plate soon made', 'soon made nut', 'made nut joke']        \n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQaPdTt9P9Rr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train=df_train.drop(['Unnamed: 0'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNW5RsE-QZGr",
        "colab_type": "code",
        "outputId": "cc6880d6-a2af-48aa-91b4-7fd7cce35d34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "df_train.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label</th>\n",
              "      <th>normalized_tweet</th>\n",
              "      <th>grams</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>['different', 'language']</td>\n",
              "      <td>['different language']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>['sue', 'legionnaire', 'family', 'affected', 'fatal', 'outbreak', 'legionnaire', 'disea']</td>\n",
              "      <td>['sue legionnaire', 'legionnaire family', 'family affected', 'affected fatal', 'fatal outbreak', 'outbreak legionnaire', 'legionnaire disea', 'sue legionnaire family', 'legionnaire family affected', 'family affected fatal', 'affected fatal outbreak', 'fatal outbreak legionnaire', 'outbreak legionnaire disea']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>['aba', 'woman', 'delivers', 'baby', 'without', 'face', 'photo']</td>\n",
              "      <td>['aba woman', 'woman delivers', 'delivers baby', 'baby without', 'without face', 'face photo', 'aba woman delivers', 'woman delivers baby', 'delivers baby without', 'baby without face', 'without face photo']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>['train', 'wreck', 'body', 'train', 'wreck', 'wreck']</td>\n",
              "      <td>['train wreck', 'wreck body', 'body train', 'train wreck', 'wreck wreck', 'train wreck body', 'wreck body train', 'body train wreck', 'train wreck wreck']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>['livestream', 'burned', 'butter', 'touched', 'hot', 'plate', 'soon', 'made', 'nut', 'joke']</td>\n",
              "      <td>['livestream burned', 'burned butter', 'butter touched', 'touched hot', 'hot plate', 'plate soon', 'soon made', 'made nut', 'nut joke', 'livestream burned butter', 'burned butter touched', 'butter touched hot', 'touched hot plate', 'hot plate soon', 'plate soon made', 'soon made nut', 'made nut joke']</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Label  ...                                                                                                                                                                                                                                                                                                                   grams\n",
              "0  1.0    ...  ['different language']                                                                                                                                                                                                                                                                                                \n",
              "1  0.0    ...  ['sue legionnaire', 'legionnaire family', 'family affected', 'affected fatal', 'fatal outbreak', 'outbreak legionnaire', 'legionnaire disea', 'sue legionnaire family', 'legionnaire family affected', 'family affected fatal', 'affected fatal outbreak', 'fatal outbreak legionnaire', 'outbreak legionnaire disea']\n",
              "2  0.0    ...  ['aba woman', 'woman delivers', 'delivers baby', 'baby without', 'without face', 'face photo', 'aba woman delivers', 'woman delivers baby', 'delivers baby without', 'baby without face', 'without face photo']                                                                                                       \n",
              "3  0.0    ...  ['train wreck', 'wreck body', 'body train', 'train wreck', 'wreck wreck', 'train wreck body', 'wreck body train', 'body train wreck', 'train wreck wreck']                                                                                                                                                            \n",
              "4  1.0    ...  ['livestream burned', 'burned butter', 'butter touched', 'touched hot', 'hot plate', 'plate soon', 'soon made', 'made nut', 'nut joke', 'livestream burned butter', 'burned butter touched', 'butter touched hot', 'touched hot plate', 'hot plate soon', 'plate soon made', 'soon made nut', 'made nut joke']        \n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cvcWehMQZKr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixWtuOxnJOtJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = (\n",
        "    tf.data.Dataset.from_tensor_slices(\n",
        "        (\n",
        "            tf.cast(df_train.iloc[:500, 1].values, tf.string),\n",
        "            tf.cast(df_train.iloc[:500, 0].values, tf.int32)\n",
        "        )\n",
        "    )\n",
        "    \n",
        ")\n",
        "\n",
        "validation_data = (\n",
        "    tf.data.Dataset.from_tensor_slices(\n",
        "        (\n",
        "            tf.cast(df_train.iloc[500:590, 1].values, tf.string),\n",
        "            tf.cast(df_train.iloc[500:590,0].values, tf.int32)\n",
        "        )\n",
        "    )\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HCcUpikKqox",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpmSD6vTQ_FO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VVLLDUXQ_Mx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#split labels from data\n",
        "data_labels = data.Label\n",
        "data_text = data.normalized_tweet"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLqijaxJS5fA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Embendings "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPUcI8CMozh1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embendding = \"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\"\n",
        "hub_layer = hub.KerasLayer(embendding, input_shape =[],\n",
        "                           dtype=tf.string, trainable = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7H69Mf0tAlQ9",
        "colab_type": "code",
        "outputId": "1121739d-61be-4be7-b894-ff637913bf69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        }
      },
      "source": [
        "from keras.layers import Dropout\n",
        "model = tf.keras.Sequential()\n",
        "model.add(hub_layer)\n",
        "model.add(tf.keras.layers.Dense(15, activation='tanh'))\n",
        "model.add(tf.keras.layers.Dense(40, activation='tanh'))\n",
        "model.add(tf.keras.layers.Dense(1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d16855aa59e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhub_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tanh'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: Current TensorFlow version is 2.2.0-rc1. To use TF 1.x instead,\nrestart your runtime (Ctrl+M .) and run \"%tensorflow_version 1.x\" before\nyou run \"import tensorflow\".\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7joP4doB2Tp",
        "colab_type": "code",
        "outputId": "4c689132-f9b4-4969-804b-22b0c92eccba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "keras_layer (KerasLayer)     (None, 20)                400020    \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 15)                315       \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 40)                640       \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 1)                 41        \n",
            "=================================================================\n",
            "Total params: 401,016\n",
            "Trainable params: 401,016\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6fm9fD2B8Qp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFTVnq4fHx5V",
        "colab_type": "code",
        "outputId": "eef212cc-9855-4fa9-f6bc-33fac63ff1e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "source": [
        "history = model.fit(train_data.shuffle(100).batch(48), epochs = 25, validation_data=validation_data.batch(24), verbose = 1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 0.6173 - accuracy: 0.6980 - val_loss: 0.5875 - val_accuracy: 0.7333\n",
            "Epoch 2/25\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.5095 - accuracy: 0.7160 - val_loss: 0.5434 - val_accuracy: 0.7333\n",
            "Epoch 3/25\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.4483 - accuracy: 0.7620 - val_loss: 0.5262 - val_accuracy: 0.7444\n",
            "Epoch 4/25\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.3921 - accuracy: 0.7940 - val_loss: 0.5208 - val_accuracy: 0.7667\n",
            "Epoch 5/25\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.3480 - accuracy: 0.8280 - val_loss: 0.5230 - val_accuracy: 0.7778\n",
            "Epoch 6/25\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.3028 - accuracy: 0.8620 - val_loss: 0.5288 - val_accuracy: 0.7778\n",
            "Epoch 7/25\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.2644 - accuracy: 0.8820 - val_loss: 0.5374 - val_accuracy: 0.7778\n",
            "Epoch 8/25\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.2282 - accuracy: 0.9000 - val_loss: 0.5495 - val_accuracy: 0.7778\n",
            "Epoch 9/25\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.2013 - accuracy: 0.9240 - val_loss: 0.5626 - val_accuracy: 0.7889\n",
            "Epoch 10/25\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1751 - accuracy: 0.9420 - val_loss: 0.5789 - val_accuracy: 0.8000\n",
            "Epoch 11/25\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1534 - accuracy: 0.9440 - val_loss: 0.5964 - val_accuracy: 0.7889\n",
            "Epoch 12/25\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1291 - accuracy: 0.9640 - val_loss: 0.6187 - val_accuracy: 0.7889\n",
            "Epoch 13/25\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1134 - accuracy: 0.9660 - val_loss: 0.6388 - val_accuracy: 0.7889\n",
            "Epoch 14/25\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0983 - accuracy: 0.9720 - val_loss: 0.6560 - val_accuracy: 0.8000\n",
            "Epoch 15/25\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0866 - accuracy: 0.9720 - val_loss: 0.6769 - val_accuracy: 0.7889\n",
            "Epoch 16/25\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0809 - accuracy: 0.9740 - val_loss: 0.6991 - val_accuracy: 0.7667\n",
            "Epoch 17/25\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0677 - accuracy: 0.9740 - val_loss: 0.7207 - val_accuracy: 0.7667\n",
            "Epoch 18/25\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0609 - accuracy: 0.9740 - val_loss: 0.7461 - val_accuracy: 0.7444\n",
            "Epoch 19/25\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0569 - accuracy: 0.9760 - val_loss: 0.7649 - val_accuracy: 0.7444\n",
            "Epoch 20/25\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0509 - accuracy: 0.9800 - val_loss: 0.7885 - val_accuracy: 0.7333\n",
            "Epoch 21/25\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0435 - accuracy: 0.9820 - val_loss: 0.8073 - val_accuracy: 0.7333\n",
            "Epoch 22/25\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0403 - accuracy: 0.9840 - val_loss: 0.8308 - val_accuracy: 0.7333\n",
            "Epoch 23/25\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0369 - accuracy: 0.9840 - val_loss: 0.8558 - val_accuracy: 0.7222\n",
            "Epoch 24/25\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0348 - accuracy: 0.9860 - val_loss: 0.8800 - val_accuracy: 0.7000\n",
            "Epoch 25/25\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0350 - accuracy: 0.9880 - val_loss: 0.9014 - val_accuracy: 0.7000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPRhPyDAQ17H",
        "colab_type": "code",
        "outputId": "4593aa97-f66d-4824-bd94-5ca2c3848541",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.predict([\"Almeida was very helpful this week. All musicians must give back to society\"])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3.5357647]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eH7UeSkqSSEk",
        "colab_type": "code",
        "outputId": "5d66e6eb-3667-4040-8237-731e336a15d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.predict([\"There was a disaster in town. Corona virus has turned the police into violent creatures.\"])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-2.5859678]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ardWGrtbVneR",
        "colab_type": "code",
        "outputId": "0348e364-70e8-4de6-cc20-a50dd0070f21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.predict([\"The girl was so beautiful and great at everything she touched. May her soul rest in peace.\"])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3.8291593]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fq4HlWIcWBmf",
        "colab_type": "code",
        "outputId": "9b455732-f2f2-48a4-9281-f571d741df35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.predict([\"This speaker had a great and inspiring academic profile.\"])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[6.369535]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYY24WFgWOcP",
        "colab_type": "code",
        "outputId": "47b9e806-275f-484b-9e0d-16a9a8873591",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.predict([\"There is no toilet paper anywhere in the shops.\"])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1.6215246]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_u-_Fpqwbkq8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}